[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Here are some of the key projects and publications I worked on:\n\nPublication: A note about Hardy’s inequality for Pseudo-Differential Operators\nProject 2: Interniship with Leonardo in the field of 3D DeepLearning"
  },
  {
    "objectID": "blog/post_18-09-24.html",
    "href": "blog/post_18-09-24.html",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "This post explores how to train a Proximal Policy Optimization (PPO) agent on the CartPole environment using the stable-baselines3 library and visualize the agent’s performance using video. The process includes training, evaluation, and generating videos for both random and trained policies.\n\n\nThe following commands are required to set up the environment in Google Colab or a similar environment. These install stable-baselines3, OpenAI Gym (now Gymnasium), and additional tools for rendering and video recording:\n!pip install stable_baselines3[extra]\n!apt-get install -y xvfb python-opengl ffmpeg\n!pip install gymnasium pyvirtualdisplay\n\n\n\nThe classic CartPole-v0 environment is used, where the task is to balance a pole on a cart by applying forces to the left or right. The environment is initialized with render_mode=“rgb_array” to capture frames for rendering videos.\nimport gymnasium as gym\nimport imageio\nfrom IPython.display import Video\n\nenv = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")\nobs, info = env.reset()\n\nframes = []\ndone = False\n\n# Capture a single episode with a random policy\nepisode = 5\nfor episode in range(1, episode+1):\n    state = env.reset()\n    done = False\n    score = 0\n\n    while not done:\n        frame = env.render()  # Capture frame by frame\n        frames.append(frame)\n        action = env.action_space.sample()  # Random action\n        obs, reward, done, _, info = env.step(action)\n\n# Save the video\nvideo_filename = '../images/blog/post_18-09-24/cartpole_random_policy.mp4'\nimageio.mimsave(video_filename, frames, fps=30)\nVideo: Random Agent\n\n\n\nThe PPO model is trained for 200,000 timesteps using stable-baselines3. The environment is vectorized using DummyVecEnv, which allows multiple environments to be run in parallel, but in this case, only one instance is used.\nimport gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Define environment\nenvironment_name = \"CartPole-v0\"\nenv = gym.make(environment_name)\n\n# Log directory\nlog_path = '/content/training/logs'\nos.makedirs(log_path, exist_ok=True)\n\n# Vectorize the environment for PPO\nenv = DummyVecEnv([lambda: gym.make(environment_name, render_mode=\"rgb_array\")])\n\n# Train PPO model\nmodel = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\nmodel.learn(total_timesteps=200000)\n\n# Save the trained model\nPPO_Path = '/content/drive/My Drive/training/saved_Models/PPO_Model_Cartpole'\nos.makedirs(PPO_Path, exist_ok=True)\nmodel.save(PPO_Path)\n\n\n\nAfter training, the model’s performance is evaluated over 10 episodes. The evaluation provides two metrics:\n\nMean reward: The average reward across the episodes.\nStandard deviation of reward: A measure of how consistent the performance is across episodes.\n\nmodel = PPO.load('/content/drive/MyDrive/training/saved_Models/PPO_Model_Cartpole.zip', env=env)\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\nprint(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n\n\nThe evaluation outputs:\nMean reward: 200.0, Std reward: 0.0\nMean reward: 200.0: This indicates that the agent has consistently achieved the maximum reward of 200 points, the upper limit for the CartPole-v0 environment. It means the agent has learned an optimal policy, balancing the pole perfectly in all episodes.\nStandard deviation (Std reward): 0.0: A standard deviation of 0.0 indicates no variation in the agent’s performance across the 10 episodes. This consistency demonstrates that the agent’s behavior is stable and reliable, without any fluctuations in performance.\n\n\n\n\nAfter training, the agent’s performance can be visualized by running it in the environment and capturing frames for a video. The following code captures the agent’s behavior for 1,000 timesteps.\n# Initialize the environment for video capture\nenv = DummyVecEnv([lambda: gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")])\nobs = env.reset()\n\nframes = []\n\n# Run the trained model and capture frames\nfor _ in range(1000):\n    action, _states = model.predict(obs)\n    obs, reward, done, info = env.step(action)\n    frames.append(env.render(mode=\"rgb_array\"))\n\n    if done:\n        obs = env.reset()\n\n# Save the video\nvideo_filename = '../images/blog/post_18-09-24/cartpole_trained_agent.mp4'\nimageio.mimsave(video_filename, frames, fps=30)\nVideo: Trained Agent\n\n\n\nThis post demonstrates how to train a PPO agent on the CartPole environment using stable-baselines3, evaluate its performance, and visualize the results through video. The trained agent achieves consistent and optimal performance, balancing the pole perfectly across all evaluation episodes. This approach can be applied to other environments and tasks for reinforcement learning research and development.\n\n\n\nThe code used in this post is adapted from Nicholas Renotte’s tutorial, “Reinforcement Learning in 3 Hours | Full Course using Python”, available on YouTube. The video provides an excellent walkthrough of training and evaluating reinforcement learning agents using stable-baselines3."
  },
  {
    "objectID": "blog/post_18-09-24.html#environment-setup",
    "href": "blog/post_18-09-24.html#environment-setup",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "The following commands are required to set up the environment in Google Colab or a similar environment. These install stable-baselines3, OpenAI Gym (now Gymnasium), and additional tools for rendering and video recording:\n!pip install stable_baselines3[extra]\n!apt-get install -y xvfb python-opengl ffmpeg\n!pip install gymnasium pyvirtualdisplay"
  },
  {
    "objectID": "blog/post_18-09-24.html#initializing-the-environment",
    "href": "blog/post_18-09-24.html#initializing-the-environment",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "The classic CartPole-v0 environment is used, where the task is to balance a pole on a cart by applying forces to the left or right. The environment is initialized with render_mode=“rgb_array” to capture frames for rendering videos.\nimport gymnasium as gym\nimport imageio\nfrom IPython.display import Video\n\nenv = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")\nobs, info = env.reset()\n\nframes = []\ndone = False\n\n# Capture a single episode with a random policy\nepisode = 5\nfor episode in range(1, episode+1):\n    state = env.reset()\n    done = False\n    score = 0\n\n    while not done:\n        frame = env.render()  # Capture frame by frame\n        frames.append(frame)\n        action = env.action_space.sample()  # Random action\n        obs, reward, done, _, info = env.step(action)\n\n# Save the video\nvideo_filename = '../images/blog/post_18-09-24/cartpole_random_policy.mp4'\nimageio.mimsave(video_filename, frames, fps=30)\nVideo: Random Agent"
  },
  {
    "objectID": "blog/post_18-09-24.html#training-the-ppo-model",
    "href": "blog/post_18-09-24.html#training-the-ppo-model",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "The PPO model is trained for 200,000 timesteps using stable-baselines3. The environment is vectorized using DummyVecEnv, which allows multiple environments to be run in parallel, but in this case, only one instance is used.\nimport gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Define environment\nenvironment_name = \"CartPole-v0\"\nenv = gym.make(environment_name)\n\n# Log directory\nlog_path = '/content/training/logs'\nos.makedirs(log_path, exist_ok=True)\n\n# Vectorize the environment for PPO\nenv = DummyVecEnv([lambda: gym.make(environment_name, render_mode=\"rgb_array\")])\n\n# Train PPO model\nmodel = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\nmodel.learn(total_timesteps=200000)\n\n# Save the trained model\nPPO_Path = '/content/drive/My Drive/training/saved_Models/PPO_Model_Cartpole'\nos.makedirs(PPO_Path, exist_ok=True)\nmodel.save(PPO_Path)"
  },
  {
    "objectID": "blog/post_18-09-24.html#evaluating-the-model",
    "href": "blog/post_18-09-24.html#evaluating-the-model",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "After training, the model’s performance is evaluated over 10 episodes. The evaluation provides two metrics:\n\nMean reward: The average reward across the episodes.\nStandard deviation of reward: A measure of how consistent the performance is across episodes.\n\nmodel = PPO.load('/content/drive/MyDrive/training/saved_Models/PPO_Model_Cartpole.zip', env=env)\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\nprint(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n\n\nThe evaluation outputs:\nMean reward: 200.0, Std reward: 0.0\nMean reward: 200.0: This indicates that the agent has consistently achieved the maximum reward of 200 points, the upper limit for the CartPole-v0 environment. It means the agent has learned an optimal policy, balancing the pole perfectly in all episodes.\nStandard deviation (Std reward): 0.0: A standard deviation of 0.0 indicates no variation in the agent’s performance across the 10 episodes. This consistency demonstrates that the agent’s behavior is stable and reliable, without any fluctuations in performance."
  },
  {
    "objectID": "blog/post_18-09-24.html#visualizing-the-trained-agent",
    "href": "blog/post_18-09-24.html#visualizing-the-trained-agent",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "After training, the agent’s performance can be visualized by running it in the environment and capturing frames for a video. The following code captures the agent’s behavior for 1,000 timesteps.\n# Initialize the environment for video capture\nenv = DummyVecEnv([lambda: gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")])\nobs = env.reset()\n\nframes = []\n\n# Run the trained model and capture frames\nfor _ in range(1000):\n    action, _states = model.predict(obs)\n    obs, reward, done, info = env.step(action)\n    frames.append(env.render(mode=\"rgb_array\"))\n\n    if done:\n        obs = env.reset()\n\n# Save the video\nvideo_filename = '../images/blog/post_18-09-24/cartpole_trained_agent.mp4'\nimageio.mimsave(video_filename, frames, fps=30)\nVideo: Trained Agent"
  },
  {
    "objectID": "blog/post_18-09-24.html#conclusion",
    "href": "blog/post_18-09-24.html#conclusion",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "This post demonstrates how to train a PPO agent on the CartPole environment using stable-baselines3, evaluate its performance, and visualize the results through video. The trained agent achieves consistent and optimal performance, balancing the pole perfectly across all evaluation episodes. This approach can be applied to other environments and tasks for reinforcement learning research and development."
  },
  {
    "objectID": "blog/post_18-09-24.html#credits",
    "href": "blog/post_18-09-24.html#credits",
    "title": "Training and Visualizing a PPO Agent on CartPole using Stable Baselines3",
    "section": "",
    "text": "The code used in this post is adapted from Nicholas Renotte’s tutorial, “Reinforcement Learning in 3 Hours | Full Course using Python”, available on YouTube. The video provides an excellent walkthrough of training and evaluating reinforcement learning agents using stable-baselines3."
  },
  {
    "objectID": "blog/post_11-09-24.html",
    "href": "blog/post_11-09-24.html",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "",
    "text": "In this post, I’ll demonstrate how to implement the K-Nearest Neighbors (KNN) algorithm from scratch in Python. We will apply the algorithm to the Iris dataset, visualize the classification results, and include images of the predicted Iris flower types."
  },
  {
    "objectID": "blog/post_11-09-24.html#overview",
    "href": "blog/post_11-09-24.html#overview",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "",
    "text": "In this post, I’ll demonstrate how to implement the K-Nearest Neighbors (KNN) algorithm from scratch in Python. We will apply the algorithm to the Iris dataset, visualize the classification results, and include images of the predicted Iris flower types."
  },
  {
    "objectID": "blog/post_11-09-24.html#the-iris-dataset",
    "href": "blog/post_11-09-24.html#the-iris-dataset",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "The Iris Dataset",
    "text": "The Iris Dataset\nThe Iris dataset consists of measurements from three types of Iris flowers: Setosa, Versicolor, and Virginica. For this demonstration, we’ll focus on Iris Setosa and Iris Versicolor.\n\nReplicating the KNN Algorithm\nHere’s a brief overview of the KNN algorithm implemented from scratch:\nimport numpy as np\nfrom collections import Counter\n\ndef distance(x, y):\n    return np.linalg.norm(x - y)\n\ndef scan_sorroundings(dataset, x, k):\n    distances = []\n    for point in dataset:\n        coord, label = point[0], point[1]\n        dist = distance(x, coord)\n        distances.append((dist, label))\n    distances.sort(key=lambda pair: pair[0])\n    return [label for _, label in distances[:k]]\n\ndef classify_new_point(dataset, x, k):\n    neighbors = scan_sorroundings(dataset, x, k)\n    most_common_label = Counter(neighbors).most_common(1)[0][0]\n    return most_common_label"
  },
  {
    "objectID": "blog/post_11-09-24.html#testing-on-the-iris-dataset",
    "href": "blog/post_11-09-24.html#testing-on-the-iris-dataset",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "Testing on the Iris Dataset",
    "text": "Testing on the Iris Dataset\nWe applied the KNN algorithm to the Iris dataset, focusing on two classes: Iris Setosa and Iris Versicolor. Here’s the code for loading the dataset and classifying a new point:\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\niris = load_iris()\ndata = iris['data']\nlabels = iris['target']\n\nbinary_data = data[labels != 2][:, :2]\nbinary_labels = labels[labels != 2]\n\ntrain_data, test_data, train_labels, test_labels = train_test_split(binary_data, binary_labels, test_size=0.2, random_state=42)\ntrain_dataset = [(train_data[i], train_labels[i]) for i in range(len(train_data))]\n\ntest_point = test_data[0]\nk = 3\nassigned_label = classify_new_point(train_dataset, test_point, k)"
  },
  {
    "objectID": "blog/post_11-09-24.html#visualizing-the-results",
    "href": "blog/post_11-09-24.html#visualizing-the-results",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nThe following code visualizes the dataset and includes an image of the predicted flower type:\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\ndef plot_knn_result_with_image(dataset, x, k, assigned_label):\n    A_points = np.array([point[0] for point in dataset if point[1] == 0])\n    B_points = np.array([point[0] for point in dataset if point[1] == 1])\n\n    plt.scatter(A_points[:, 0], A_points[:, 1], color='red', label='Iris Setosa')\n    plt.scatter(B_points[:, 0], B_points[:, 1], color='blue', label='Iris Versicolor')\n\n    if assigned_label == 0:\n        plt.scatter(x[0], x[1], color='red', marker='x', s=100)\n        image_path = 'iris_setosa.jpg'\n    else:\n        plt.scatter(x[0], x[1], color='blue', marker='x', s=100)\n        image_path = 'iris_versicolor.jpg'\n\n    plt.legend()\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.title(f'KNN Classification (k={k})')\n    plt.show()\n\n    img = mpimg.imread(image_path)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(f'Predicted Flower: {\"Iris Setosa\" if assigned_label == 0 else \"Iris Versicolor\"}')\n    plt.show()\n\nplot_knn_result_with_image(train_dataset, test_point, k, assigned_label)"
  },
  {
    "objectID": "blog/post_11-09-24.html#plot-of-a-classification-result",
    "href": "blog/post_11-09-24.html#plot-of-a-classification-result",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "Plot of a classification result",
    "text": "Plot of a classification result\n\n\n\nFigure 1: Classification of a test point"
  },
  {
    "objectID": "blog/post_11-09-24.html#the-two-species-of-iris-classified",
    "href": "blog/post_11-09-24.html#the-two-species-of-iris-classified",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "The two species of iris classified",
    "text": "The two species of iris classified\n\n\n\n\n\n\nFigure 2: Iris Versicolor\n\n\n\n\n\n\n\nFigure 3: Iris Setosa\n\n\n\n\n\n\nPhoto of Iris Versicolor (Blue flag flower) by Danielle Langlois, taken in July 2005 at Forillon National Park, Quebec, Canada. License: CC BY-SA 3.0. File.\n\n\nPhoto of Iris Setosa by Radomil. License: GNU Free Documentation License, Version 1.2 or later."
  },
  {
    "objectID": "blog/post_11-09-24.html#credits",
    "href": "blog/post_11-09-24.html#credits",
    "title": "Implementing K-Nearest Neighbors (KNN) from Scratch Using the Iris Dataset",
    "section": "Credits",
    "text": "Credits\n\nK-Nearest Neighbors Algorithm:\nThis implementation was adapted from the tutorial provided by Jason Brownlee, “Tutorial to Implement k-Nearest Neighbors in Python from Scratch.”\nSource: Machine Learning Mastery\nIris Dataset:\nThe dataset used in this post is the famous Iris dataset. It is available through the Scikit-learn library.\nVisualizations:\nVisualizations were created using the Matplotlib library."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Welcome to My Site",
    "section": "Recent Posts",
    "text": "Recent Posts\n\nTraining and Visualizing a PPO Agent on CartPole using Stable Baselines3\nImplementing K-Nearest Neighbor with Python"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts by Date:",
    "section": "",
    "text": "September 11, 2024: Implementing K-Nearest Neighbor with Python\n\n\nSeptember 18, 2024: Implementing K-Nearest Neighbor with Python"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I graduated with a degree in Mathematics in October 2022. My thesis, titled La teoria di Payley Littlewood e alcune sue applicazioni in analisi, focused on Harmonic Analysis and resulted in a published article in the Bollettino Unione Matematica Italiana. In the article, “A Note on the Fractional Hardy Inequality”, my thesis supervisor and I present a minor enhancement of Hardy’s inequality. Since graduation, I have developed an interest in machine learning, completing online courses including Andrew Ng’s Machine Learning Specialization.\n\n\n\nIn 2023, I began the Master in Mathematical and Physical Methods program, where I am expanding my expertise in mathematical modeling techniques.\n\n\n\nDuring the MPM Master, I undertook a curricular internship at Leonardo Divisione Velivoli, Corso Francia, 426, 10146 Torino, where I worked on computer vision and 3D deep learning. This role provided me with hands-on experience in cutting-edge technologies and allowed me to apply my mathematical knowledge to practical problems.\n\n\n\n\nArticle: “A Note on the Fractional Hardy Inequality” published in Bollettino Unione Matematica Italiana (2023): Read the article\n\n\n\n\nYou can reach me via email or connect with me on LinkedIn.\n\n\n\nMy CV is available for download here."
  },
  {
    "objectID": "about.html#academic-background",
    "href": "about.html#academic-background",
    "title": "About Me",
    "section": "",
    "text": "I graduated with a degree in Mathematics in October 2022. My thesis, titled La teoria di Payley Littlewood e alcune sue applicazioni in analisi, focused on Harmonic Analysis and resulted in a published article in the Bollettino Unione Matematica Italiana. In the article, “A Note on the Fractional Hardy Inequality”, my thesis supervisor and I present a minor enhancement of Hardy’s inequality. Since graduation, I have developed an interest in machine learning, completing online courses including Andrew Ng’s Machine Learning Specialization."
  },
  {
    "objectID": "about.html#current-studies",
    "href": "about.html#current-studies",
    "title": "About Me",
    "section": "",
    "text": "In 2023, I began the Master in Mathematical and Physical Methods program, where I am expanding my expertise in mathematical modeling techniques."
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "",
    "text": "During the MPM Master, I undertook a curricular internship at Leonardo Divisione Velivoli, Corso Francia, 426, 10146 Torino, where I worked on computer vision and 3D deep learning. This role provided me with hands-on experience in cutting-edge technologies and allowed me to apply my mathematical knowledge to practical problems."
  },
  {
    "objectID": "about.html#selected-publications-and-projects",
    "href": "about.html#selected-publications-and-projects",
    "title": "About Me",
    "section": "",
    "text": "Article: “A Note on the Fractional Hardy Inequality” published in Bollettino Unione Matematica Italiana (2023): Read the article"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "",
    "text": "You can reach me via email or connect with me on LinkedIn."
  },
  {
    "objectID": "about.html#download-cv",
    "href": "about.html#download-cv",
    "title": "About Me",
    "section": "",
    "text": "My CV is available for download here."
  }
]